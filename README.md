# TuneAhead (Anonymous Repo)

## ðŸ“¦ Installation

```bash
git clone https://anonymous.4open.science/r/tuneahead-anonymous.git
cd tuneahead-anonymous
pip install -r requirements.txt
```

## ðŸ“‚ Repository Structure

```
TuneAhead/
â”œâ”€â”€ requirements.txt 
â”œâ”€â”€ features/              # Feature extraction modules
â”‚   â”œâ”€â”€ extract_static.py  # Static dataset descriptors
â”‚   â”œâ”€â”€ extract_dynamic.py # Dynamic probe signals
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ run.sh                  # Example script for training & evaluation
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ main.yaml           # Default configuration
â”œâ”€â”€ data/
â”‚   â””â”€â”€ README_DATA.md
â”‚   â”œâ”€â”€ sample_metadataset.csv 
â””â”€â”€ src/
    â”œâ”€â”€ io_utils.py         # Data I/O helpers
    â”œâ”€â”€ utils.py            # General utilities
    â”œâ”€â”€ splitters.py        # Dataset splitting
    â”œâ”€â”€ metrics.py          # Evaluation metrics
    â”œâ”€â”€ conformal.py        # (Optional) conformal calibration
    â”œâ”€â”€ plots.py            # Visualization helpers (grid, SHAP, calibration)
    â”œâ”€â”€ train.py            # Main training script
    â”œâ”€â”€ models/
    â”‚   â”œâ”€â”€ lightgbm_reg.py # Gradient boosting predictor
    â”‚   â”œâ”€â”€ baselines.py    # Baseline models (ProxyLM, scaling-law, etc.)
    â”‚   â””â”€â”€ calibrate.py    # Isotonic/Platt calibration
    â”œâ”€â”€ features/
    â”‚   â”œâ”€â”€ build_views.py  # Dynamic probe features
    â”‚   â””â”€â”€ static_rules.py # Static dataset descriptors
    â””â”€â”€ interpret/
        â””â”€â”€ shap_global.py  # SHAP-based interpretability
â””â”€â”€ README.md
```

## ðŸ“Š Data

We release a randomly sampled subset of the meta-dataset under `data/`.  
All subsets are stored in **CSV format** and can be directly consumed by the training and evaluation scripts.

Full-scale datasets and private ETL pipelines are **not included** due to license/privacy constraints and ongoing research.  

All interfaces remain identical, so replacing the subset CSVs with full datasets will work without modification.

## ðŸ›  Feature Extraction

Code for feature extraction is under `features/`:

- `extract_static.py`: computes dataset-intrinsic descriptors (e.g., lexical diversity, token length statistics, duplication rate).  
- `extract_dynamic.py`: runs fixed-budget probe fine-tunes to capture early dynamics (e.g., loss decay, gradient stability).  

Both modules output CSV files that match the input format expected by `train.py`.

## ðŸš€ Training & Evaluation
Example usage (see run.sh for more):
```bash
# Train predictor
bash run.sh
```

## ðŸ“ˆ Visualization

Visualization scripts are under `plots/`:

- Grid plots of predicted vs. true accuracy  
- SHAP feature attribution plots  
- Calibration and isotonic regression curves  

Run, for example:

```bash
python plots/plot_grid.py --input results/predictions.csv
```

---

## ðŸ“œ License & Notes

- Full-scale meta-datasets are **not released** due to license/privacy constraints and because they remain part of ongoing research.  
- All released code is anonymized and stripped of sensitive components for ICLR review.  
