# TuneAhead (Anonymous Repo)

## ðŸ“¦ Installation

```bash
git clone https://anonymous.4open.science/r/tuneahead-anonymous.git
cd tuneahead-anonymous
pip install -r requirements.txt
```

## ðŸ“‚ Repository Structure

```
TuneAhead/
â”œâ”€â”€ requirements.txt 
â”œâ”€â”€ preprocessing/              # Get Ground Truth and Feature extraction
â”‚   â”œâ”€â”€ Feature Extraction Layer
â”‚        â”œâ”€â”€ feature_pipeline.py       # Unified pipeline
â”‚        â”œâ”€â”€ static_features.py        # Static features
â”‚        â”œâ”€â”€ dynamic_probes.py         # Dynamic features
â”‚        â””â”€â”€ pipe_folder_to_csv_isolated.py  # Isolated processing
â”‚   â”œâ”€â”€ batch_mmlu_eval.py      # Batch MMLU evaluation controller
â”‚   â”œâ”€â”€ auto_mmlu_eval.py      # Automated experiment orchestrator
â”‚   â”œâ”€â”€ evaluate_model.py      # Model evaluation module
â”‚   â””â”€â”€ train_model.py         # LoRA training module
â”œâ”€â”€ run.sh                  # Example script for training & evaluation
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ main.yaml           # Default configuration
â”œâ”€â”€ data/
â”‚   â””â”€â”€ README_DATA.md
â”‚   â”œâ”€â”€ metadataset_sample.csv 
â””â”€â”€ src/
    â”œâ”€â”€ io_utils.py         # Data I/O helpers
    â”œâ”€â”€ utils.py            # General utilities
    â”œâ”€â”€ splitters.py        # Dataset splitting
    â”œâ”€â”€ metrics.py          # Evaluation metrics
    â”œâ”€â”€ conformal.py        # (Optional) conformal calibration
    â”œâ”€â”€ plots.py            # Visualization helpers (grid, SHAP, calibration)
    â”œâ”€â”€ train.py            # Main training script
    â”œâ”€â”€ models/
    â”‚   â”œâ”€â”€ lightgbm_reg.py # Gradient boosting predictor
    â”‚   â”œâ”€â”€ baselines.py    # Baseline models (ProxyLM, scaling-law, etc.)
    â”‚   â””â”€â”€ calibrate.py    # Isotonic/Platt calibration
    â”œâ”€â”€ features/
    â”‚   â”œâ”€â”€ build_views.py  # Build design matrices (X_all/X_proxy/y) from merged CSV; no feature computation
    â”‚   â””â”€â”€ static_rules.py # Heuristically partition columns into static/dynamic/hyper groups for ablations
    â””â”€â”€ interpret/
        â””â”€â”€ shap_global.py  # SHAP-based interpretability
â””â”€â”€ README.md
```

## ðŸ“Š Data

We release a randomly sampled subset of the meta-dataset under `data/`.  
All subsets are stored in **CSV format** and can be directly consumed by the training and evaluation scripts.

Full-scale datasets and private ETL pipelines are **not included** due to license/privacy constraints and ongoing research.  

All interfaces remain identical, so replacing the subset CSVs with full datasets will work without modification.

## ðŸ›  Feature Extraction

Code for feature extraction is under `features/`:

- `extract_static.py`: computes dataset-intrinsic descriptors (e.g., lexical diversity, token length statistics, duplication rate).  
- `extract_dynamic.py`: runs fixed-budget probe fine-tunes to capture early dynamics (e.g., loss decay, gradient stability).  

Both modules output CSV files that match the input format expected by `train.py`.

## ðŸš€ Training & Evaluation
Example usage (see run.sh for more):
```bash
# Train predictor
bash run.sh
```

## ðŸ“ˆ Visualization

Visualization scripts are under `plots/`:

- Grid plots of predicted vs. true accuracy  
- SHAP feature attribution plots  
- Calibration and isotonic regression curves  

Run, for example:

```bash
python plots/plot_grid.py --input results/predictions.csv
```

---

## ðŸ”® Using Pretrained Model

We provide a pretrained **TuneAhead (Full)** LightGBM model in `models/model_TuneAhead_Full.txt`,  
which can be used to directly predict fine-tuning performance for new datasets.

### 1. Prepare your dataset
- Input format: **CSV** file containing the same static and dynamic features used in our paper.  
- You may optionally include identifier columns (e.g., `dataset_name`, `run_id`) and ground-truth labels (e.g., `overall_accuracy`) for comparison.  
- Example: `data/sample_metadataset.csv`

### 2. Run prediction
```bash
# Minimal usage: only specify your CSV file
python src/predict.py --data-csv data/sample_metadataset.csv

# With additional options
python src/predict.py \
  --model-txt models/model_TuneAhead_Full.txt \
  --data-csv data/sample_metadataset.csv \
  --id-cols dataset_name run_id \
  --label-col overall_accuracy \
  --out-csv results/predictions.csv
## ðŸ“œ License & Notes

- Full-scale meta-datasets are **not released** due to license/privacy constraints and because they remain part of ongoing research.  
- All released code is anonymized and stripped of sensitive components for ICLR review.  
